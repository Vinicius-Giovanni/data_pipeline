# ğŸ“¦ Data Pipeline

Projeto robusto de pipelines de dados para tratamento, transformaÃ§Ã£o e anÃ¡lise de dados logÃ­sticos. Estruturado de forma modular, com foco em reutilizaÃ§Ã£o de cÃ³digo, logging centralizado e escalabilidade, o **Data Pipeline** viabiliza a ingestÃ£o, limpeza, processamento e integraÃ§Ã£o de dados operacionais, com suporte a mÃºltiplos fluxos simultÃ¢neos.

## ğŸ“ Estrutura do Projeto

data_pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                 # ConfiguraÃ§Ãµes fixas e variÃ¡veis globais
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # CSVs puros de entrada
â”‚   â”‚   â”œâ”€â”€ data_olpn.csv
â”‚   â”‚   â””â”€â”€ data_picking.csv
â”‚   â”‚   â””â”€â”€ data_packing.csv
â”‚   â”‚   â””â”€â”€ data_cancel.csv
â”‚   â”‚   â””â”€â”€ data_load.csv
â”‚   â”‚   â””â”€â”€ data_putaway.csv
â”‚   â”‚   â””â”€â”€ data_jornada.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                  # CSVs limpos e transformados
â”‚   â”‚   â”œâ”€â”€ data_olpn.csv
â”‚   â”‚   â””â”€â”€ data_picking.csv
â”‚   â”‚   â””â”€â”€ data_packing.csv
â”‚   â”‚   â””â”€â”€ data_cancel.csv
â”‚   â”‚   â””â”€â”€ data_load.csv
â”‚   â”‚   â””â”€â”€ data_putaway.csv
â”‚   â”‚   â””â”€â”€ data_jornada.csv
â”‚   â”‚
â”‚   â””â”€â”€ parquet/                    # Arquivos Parquet finais
â”‚       â”œâ”€â”€ data_olpn.parquet
â”‚       â””â”€â”€ data_picking.parquet
â”‚       â””â”€â”€ data_packing.parquet
â”‚       â””â”€â”€ data_cancel.parquet
â”‚       â””â”€â”€ data_load.parquet
â”‚       â””â”€â”€ data_putaway.parquet
â”‚       â””â”€â”€ data_jornada.parquet
â”‚
â”œâ”€â”€ integrated_pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bottleneck_box.py           # Pipeline especÃ­fico para anÃ¡lise de bottleneck box
â”‚   â”œâ”€â”€ bottleneck_salao.py         # Pipeline especÃ­fico para anÃ¡lise de bottleneck salÃ£o
â”‚   â”œâ”€â”€ jornada_pipeline.py         # Pipeline especÃ­fico para anÃ¡lise de jornada
â”‚   â””â”€â”€ time_lead_olpn.py
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log                # SaÃ­da de logs centralizada
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_pipeline.py            # Classe base para pipelines
â”‚   â”œâ”€â”€ olpn_pipeline.py            # Pipeline especÃ­fico do OLPn
â”‚   â”œâ”€â”€ picking_pipeline.py         # Pipeline especÃ­fico do Picking
â”‚   â”œâ”€â”€ cancel_pipeline.py          # Pipeline especÃ­fico do Cancel
â”‚   â”œâ”€â”€ jornada_pipeline.py         # Pipeline especÃ­fico da Jornada
â”‚   â”œâ”€â”€ load_pipeline.py            # Pipeline especÃ­fico do Load
â”‚   â”œâ”€â”€ packing_pipeline.py         # Pipeline especÃ­fico do Packing
â”‚   â”œâ”€â”€ putaway_pipeline.py         # Pipeline especÃ­fico do Putaway
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ io.py                       # Leitura/escrita de arquivos
â”‚   â”œâ”€â”€ transformation.py           # FunÃ§Ãµes de transformaÃ§Ã£o
â”‚   â”œâ”€â”€ classification.py           # LÃ³gicas de classificaÃ§Ã£o
â”‚   â””â”€â”€ logging_utils.py            # ConfiguraÃ§Ã£o de logs
â”‚                  
â”œâ”€â”€ process_pipeline.py             # Script principal que roda todos os pipelines
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                       # ExplicaÃ§Ãµes do projeto

---

## ğŸ¯ Objetivo

Automatizar o tratamento e anÃ¡lise de dados operacionais logÃ­sticos, garantindo consistÃªncia, rastreabilidade e performance no fluxo de ponta a ponta, desde a ingestÃ£o de arquivos brutos atÃ© a geraÃ§Ã£o de arquivos Parquet prontos para anÃ¡lise em ferramentas de BI.

---

## VisÃ£o Geral

### MÃ³dulo `settings.py`

Este mÃ³dulo centraliza as configuraÃ§Ãµes globais e especÃ­ficas de cada pipeline do projeto **Data Pipeline**, incluindo caminhos de dados, parÃ¢metros de leitura e escrita, definiÃ§Ãµes de tipos de dados, regras de transformaÃ§Ã£o e mapeamentos de negÃ³cio. Atua como fonte Ãºnica de verdade para os pipelines, garantindo coerÃªncia e facilidade de manutenÃ§Ã£o.

---

### MÃ³dulo `bottleneck_box.py`

O mÃ³dulo `bottleneck_box.py` implementa o pipeline dedicado Ã  anÃ¡lise do gargalo operacional denominado "bottleneck box". Utilizando leitura eficiente de arquivos Parquet particionados, o pipeline integra dados dos estÃ¡gios de `load` e `putaway`, aplicando regras de tratamento de dados configuradas centralmente para garantir padronizaÃ§Ã£o e qualidade.

A lÃ³gica central consiste em combinar os datasets via chave Ãºnica (`olpn`), realizar conversÃµes temporais para cÃ¡lculo do tempo entre eventos crÃ­ticos, e exportar os resultados segmentados por mÃªs para facilitar anÃ¡lises temporais e geraÃ§Ã£o de relatÃ³rios. Todo processo Ã© monitorado via logging robusto, assegurando rastreabilidade e facilidade de diagnÃ³stico em ambiente produtivo.

---

### MÃ³dulo `bottleneck_salao.py`

O mÃ³dulo `bottleneck_salao.py` implementa o pipeline voltado para anÃ¡lise do gargalo operacional denominado "bottleneck salÃ£o". O pipeline realiza a leitura eficiente de arquivos Parquet particionados referentes Ã s etapas de `packed` e `putaway`, consolidando os dados com base na chave Ãºnica `olpn`.

AtravÃ©s da uniÃ£o dos datasets e conversÃ£o de colunas temporais, calcula o intervalo entre eventos crÃ­ticos para identificar e mensurar o gargalo. Os resultados sÃ£o exportados em arquivos segmentados mensalmente, facilitando anÃ¡lises temporais e o monitoramento contÃ­nuo. Todo o processo Ã© suportado por um sistema de logging estruturado, garantindo rastreabilidade e confiabilidade operacional.

---

### MÃ³dulo `jornada_pipeline.py`

O mÃ³dulo `jornada_pipeline.py` implementa o pipeline focado no processamento de dados de jornada de trabalho. Ele realiza o prÃ©-processamento estruturado de arquivos CSV, aplicando limpeza, padronizaÃ§Ã£o e enriquecimento dos dados conforme configuraÃ§Ãµes especÃ­ficas do pipeline.

AlÃ©m disso, converte arquivos CSV processados para o formato Parquet, otimizando armazenamento e performance para anÃ¡lises subsequentes. O pipeline suporta segmentaÃ§Ã£o dos dados por mÃªs para facilitar a gestÃ£o temporal e conta com logging detalhado para garantir monitoramento e rÃ¡pida identificaÃ§Ã£o de falhas durante a execuÃ§Ã£o.

---

### MÃ³dulo `time_lead_olpn.py`

O mÃ³dulo `time_lead_olpn.py` implementa o pipeline responsÃ¡vel pela anÃ¡lise do lead time do processo OLPN. Ele realiza a leitura de mÃºltiplos arquivos Parquet, aplicando prÃ©-processamento com validaÃ§Ã£o, limpeza, tratamento de tipos e cÃ¡lculo do tempo absoluto entre eventos-chave (`data_hora_load` e `data_pedido`).

Os dados sÃ£o segmentados e exportados por mÃªs, garantindo granularidade temporal para anÃ¡lises operacionais e de performance. A soluÃ§Ã£o incorpora logging avanÃ§ado para rastreamento detalhado, assegurando transparÃªncia e controle durante toda a execuÃ§Ã£o do pipeline.

---

### MÃ³dulo `base_pipeline.py`

O mÃ³dulo `base_pipeline.py` define a classe abstrata `BasePipeline`, que serve como estrutura base para a implementaÃ§Ã£o dos pipelines especÃ­ficos do projeto. Ela encapsula a lÃ³gica central de leitura de arquivos CSV, prÃ©-processamento, validaÃ§Ã£o e exportaÃ§Ã£o dos dados segmentados por mÃªs, tanto em CSV quanto em Parquet.

A classe promove padronizaÃ§Ã£o e reutilizaÃ§Ã£o, exigindo que subclasses implementem o mÃ©todo `preprocess` para aplicar regras especÃ­ficas de transformaÃ§Ã£o. O uso de logging robusto assegura controle operacional e fÃ¡cil diagnÃ³stico durante a execuÃ§Ã£o dos pipelines.

---

### MÃ³dulo `cancel_pipeline.py`

Este mÃ³dulo implementa o `CancelPipeline`, uma extensÃ£o da classe base `BasePipeline`, dedicada ao processamento dos dados de cancelamento.

A pipeline executa etapas estruturadas de limpeza e transformaÃ§Ã£o, incluindo:
- RemoÃ§Ã£o e renomeaÃ§Ã£o de colunas conforme configuraÃ§Ã£o especÃ­fica.
- ConversÃ£o de tipos de dados e normalizaÃ§Ã£o de datas.
- ClassificaÃ§Ã£o dos setores com funÃ§Ã£o customizada `cancel_classify_setores`.
- Tratamento avanÃ§ado da coluna `motivo_cancelamento`, aplicando normalizaÃ§Ã£o textual (remoÃ§Ã£o de acentos, padronizaÃ§Ã£o de caixa) e categorizaÃ§Ã£o via a funÃ§Ã£o `normalizar_motivo`.
- GeraÃ§Ã£o de colunas auxiliares para anÃ¡lise temporal (`hora`, `data_criterio`, `mes_ano`) baseadas na data do cancelamento.

Essa arquitetura promove governanÃ§a robusta, assegurando qualidade e consistÃªncia dos dados para anÃ¡lises avanÃ§adas e dashboards corporativos.

---

### MÃ³dulo `load_pipeline.py`

Este mÃ³dulo define o `LoadPipeline`, uma especializaÃ§Ã£o da classe base `BasePipeline` focada no tratamento dos dados de carregamento (`load`).

O processo de prÃ©-processamento contempla:

- Limpeza estruturada removendo e renomeando colunas conforme configuraÃ§Ã£o centralizada.
- ConversÃ£o rigorosa de tipos e tratamento de colunas temporais.
- ExtraÃ§Ã£o da data base a partir da coluna `data_hora_load`.
- CÃ¡lculo do tempo total de atividade diÃ¡ria por box (`duracao_segundos_box_dia`), agregando as mÃ­nimas e mÃ¡ximas marcaÃ§Ãµes temporais por combinaÃ§Ã£o de box e data.
- GeraÃ§Ã£o de colunas temporais padronizadas para anÃ¡lises granulares (`hora`, `data_criterio`, `mes_ano`).
- ClassificaÃ§Ã£o dos setores com funÃ§Ã£o dedicada `classify_setores`.
- Limpeza final removendo colunas auxiliares resultantes de merges intermediÃ¡rios.

A pipeline reforÃ§a a governanÃ§a dos dados operacionais, garantindo a consistÃªncia e enriquecimento temporal para anÃ¡lises avanÃ§adas e monitoramento estratÃ©gico.  

---

### MÃ³dulo `olpn_pipeline.py`

Este mÃ³dulo implementa a classe `OlpnPipeline`, derivada de `BasePipeline`, destinada ao prÃ©-processamento dos dados OPLN (Order Picking Location Number).

Pontos-chave do processamento:

- RemoÃ§Ã£o e renomeaÃ§Ã£o de colunas conforme configuraÃ§Ã£o centralizada.
- ConversÃ£o robusta de tipos, incluindo tratamento especial para a coluna `volume` com conversÃ£o numÃ©rica e substituiÃ§Ã£o de vÃ­rgulas.
- ExtraÃ§Ã£o detalhada do campo `local_de_picking` em trÃªs componentes (`rua`, `endereco` e `nivel`), facilitando anÃ¡lises granulares de localizaÃ§Ã£o.
- ClassificaÃ§Ã£o de setores por meio da funÃ§Ã£o `classify_setores`.
- ConstruÃ§Ã£o da coluna `localizacao` com lÃ³gica condicional baseada em valores especÃ­ficos, distinguindo entre Ã¡rea "P.A.R" e "Salao".
- DefiniÃ§Ã£o de partiÃ§Ãµes temporais (`mes_ano`, `data_criterio`, `hora`) fundamentadas na coluna `data_hora_ultimo_update_olpn`, com fallback controlado para ausÃªncia desse dado.

Essa pipeline Ã© essencial para estruturar, enriquecer e segmentar os dados logÃ­sticos, habilitando anÃ¡lises espaciais e temporais avanÃ§adas dentro do contexto operacional.

---

### MÃ³dulo `packing_pipeline.py`

Este mÃ³dulo implementa a classe `PackingPipeline`, que herda de `BasePipeline` e Ã© responsÃ¡vel pelo processamento dos dados de packing, com integraÃ§Ã£o da referÃªncia OLPN para enriquecer o dataset.

Destaques do processamento:

- Carregamento da base OLPN via funÃ§Ã£o especializada `read_parquet_with_tote` para integraÃ§Ã£o de dados.
- AplicaÃ§Ã£o de transformaÃ§Ãµes configuradas, incluindo remoÃ§Ã£o e renomeaÃ§Ã£o de colunas, ajuste de tipos e normalizaÃ§Ã£o textual da coluna `desc_setor_item`.
- Merge entre os dados de packing e a referÃªncia OLPN com chave `olpn`, agregando informaÃ§Ãµes cruciais.
- CÃ¡lculo do tempo de atividade diÃ¡rio por `box` e por `tote` baseado em timestamps, gerando mÃ©tricas de duraÃ§Ã£o em segundos.
- Ajuste para garantir que duraÃ§Ãµes de tote menores que 60 segundos sejam normalizadas para 60 segundos, assegurando consistÃªncia analÃ­tica.
- ExtraÃ§Ã£o e formataÃ§Ã£o de partiÃ§Ãµes temporais (`hora`, `data_criterio`, `mes_ano`) a partir da coluna `data_hora_packed`.
- ClassificaÃ§Ã£o de setores para segmentaÃ§Ã£o operacional.
- Limpeza final dos dados para eliminar colunas intermediÃ¡rias irrelevantes.

Essa pipeline garante uma visÃ£o integrada e temporalmente segmentada do processo de packing, essencial para anÃ¡lises de eficiÃªncia operacional e controle logÃ­stico.

---

### MÃ³dulo `picking_pipeline.py`

Este mÃ³dulo define a classe `PickingPipeline`, especializada no processamento dos dados de picking, derivando da classe base `BasePipeline`.

Principais funcionalidades e transformaÃ§Ãµes:

- AplicaÃ§Ã£o de limpeza e padronizaÃ§Ã£o via remoÃ§Ã£o e renomeaÃ§Ã£o de colunas conforme configuraÃ§Ã£o.
- ConversÃ£o de tipos de dados e parsing de datas para garantir integridade e coerÃªncia temporal.
- CÃ¡lculo do tempo de duraÃ§Ã£o da tarefa (`duracao_tarefa_segundos`) a partir das colunas de inÃ­cio e fim da tarefa.
- Desmembramento da coluna `local_de_picking` em subcomponentes (`rua`, `endereco`, `nivel`) para granularidade espacial.
- Preenchimento de valores ausentes nos campos de localizaÃ§Ã£o para evitar inconsistÃªncias.
- ClassificaÃ§Ã£o da localizaÃ§Ã£o em categorias operacionais ("P.A.R" ou "Salao") baseada em regras definidas para ruas e endereÃ§os.
- AtribuiÃ§Ã£o de setores via funÃ§Ã£o de classificaÃ§Ã£o especÃ­fica, promovendo segmentaÃ§Ã£o analÃ­tica.
- GeraÃ§Ã£o de colunas temporais de particionamento (`mes_ano`, `data_criterio`, `hora`) baseadas no timestamp de tÃ©rmino da tarefa.
- Logging de alertas quando informaÃ§Ãµes essenciais para particionamento temporal estÃ£o ausentes.

Esta pipeline promove a anÃ¡lise detalhada do desempenho e eficiÃªncia das operaÃ§Ãµes de picking, com foco em segmentaÃ§Ã£o espacial e temporal para suportar decisÃµes estratÃ©gicas e tÃ¡ticas.

---

### MÃ³dulo `putaway_pipeline.py`

Este mÃ³dulo implementa a classe `PutawayPipeline`, especializada no processamento dos dados de putaway, estendendo a estrutura do `BasePipeline`.

Pontos-chave da pipeline:

- AplicaÃ§Ã£o de limpeza, remoÃ§Ã£o e renomeaÃ§Ã£o de colunas conforme definido nas configuraÃ§Ãµes especÃ­ficas.
- ConversÃ£o robusta de tipos, incluindo extraÃ§Ã£o numÃ©rica da coluna `box` para padronizaÃ§Ã£o.
- Parsing e validaÃ§Ã£o das colunas de data/hora para garantir coerÃªncia temporal.
- CÃ¡lculo do tempo total por box e dia, a partir das timestamps mÃ­nimas e mÃ¡ximas, refletindo a duraÃ§Ã£o operacional diÃ¡ria.
- IntegraÃ§Ã£o dos cÃ¡lculos de tempo com o DataFrame principal via merge, para anÃ¡lise consolidada.
- ConstruÃ§Ã£o de colunas para particionamento temporal (`hora`, `data_criterio`, `mes_ano`) com base no timestamp de putaway.
- ClassificaÃ§Ã£o dos dados em setores operacionais por meio da funÃ§Ã£o `classify_setores`.
- Limpeza final removendo colunas temporÃ¡rias ou redundantes para otimizar o dataset.
- Logging para monitoramento do fluxo e alertas sobre ausÃªncia de dados crÃ­ticos.

Esta pipeline Ã© essencial para mensurar e analisar a eficiÃªncia das operaÃ§Ãµes de putaway, suportando decisÃµes que impactam diretamente a produtividade e a gestÃ£o do fluxo logÃ­stico.

---

### MÃ³dulo `classification.py`

Este mÃ³dulo contÃ©m funÃ§Ãµes essenciais para classificaÃ§Ã£o setorial dos dados operacionais, fundamental para segmentaÃ§Ã£o analÃ­tica e tomada de decisÃ£o orientada por contexto.

### FunÃ§Ã£o `classify_setores`

- Recebe um DataFrame com informaÃ§Ãµes de pedidos e boxes.
- Aplica mÃºltiplas condiÃ§Ãµes lÃ³gicas compostas por combinaÃ§Ãµes de `tipo_de_pedido` e faixa numÃ©rica de `box`.
- Cada condiÃ§Ã£o associa um rÃ³tulo setorial especÃ­fico (ex: "Fracionado Pesados", "EAD - Abastecimento de Lojas", "Ribeirao Preto", etc.).
- Converte a coluna `box` para inteiro, tratando valores invÃ¡lidos.
- Retorna uma sÃ©rie com a classificaÃ§Ã£o setorial baseada nas condiÃ§Ãµes.
- Valor default para registros que nÃ£o atendem nenhuma condiÃ§Ã£o Ã© "Outras Saidas".

### FunÃ§Ã£o `cancel_classify_setores`

- Similar Ã  funÃ§Ã£o anterior, mas focada em classificaÃ§Ã£o para dados de cancelamento.
- Avalia apenas o campo `tipo_de_pedido` para atribuir os setores.
- TambÃ©m retorna uma sÃ©rie categorizada, com default "Outras saidas".

### MÃ³dulo `io.py`

Este mÃ³dulo Ã© responsÃ¡vel pela leitura eficiente de arquivos de entrada nos pipelines, garantindo resiliÃªncia, performance e consistÃªncia com as configuraÃ§Ãµes de cada fluxo de dados.

### FunÃ§Ã£o: `read_all_csv`

**Objetivo:**  
Ler todos os arquivos `.csv` de um diretÃ³rio especÃ­fico, utilizando `chunks` para eficiÃªncia de memÃ³ria.

**ParÃ¢metros:**
- `folder: Path` â†’ Caminho para a pasta contendo os arquivos `.csv`.
- `pipeline_key: str` â†’ Chave de identificaÃ§Ã£o do pipeline no `PIPELINE_CONFIG`.

**Funcionamento:**
- Recupera configuraÃ§Ãµes especÃ­ficas de encoding e separador do pipeline.
- Itera por todos os arquivos `.csv` no diretÃ³rio, utilizando `pd.read_csv()` com `chunksize` (carregamento em blocos).
- Todos os chunks sÃ£o concatenados ao final em um Ãºnico `DataFrame`.
- Em caso de erro durante a leitura de qualquer arquivo, registra no log e prossegue com os demais.

**Retorno:**
- `pd.DataFrame` consolidado contendo os dados de todos os arquivos `.csv`.

### FunÃ§Ã£o: `read_parquet_with_tote`

**Objetivo:**  
Ler mÃºltiplos arquivos `.parquet` contendo apenas as colunas `olpn` e `tote`.

**ParÃ¢metros:**
- `folder: Path` â†’ Caminho da pasta com arquivos `.parquet`.

**Funcionamento:**
- Itera sobre os arquivos `.parquet` da pasta.
- Para cada arquivo, lÃª somente as colunas `['olpn', 'tote']`.
- Concatena os `DataFrames` e remove duplicatas com base em `olpn`.

**Retorno:**
- `pd.DataFrame` Ãºnico contendo os pares `olpn`-`tote` sem duplicaÃ§Ãµes.

### ObservaÃ§Ãµes EstratÃ©gicas

- O uso de leitura por chunks (`CHUNK_SIZE`) no `read_all_csv` Ã© uma prÃ¡tica robusta para evitar estouro de memÃ³ria em arquivos extensos.
- Ambos os mÃ©todos sÃ£o projetados para integrar-se diretamente ao sistema de configuraÃ§Ã£o modular `PIPELINE_CONFIG`, conferindo escalabilidade e adaptabilidade.
- O `read_parquet_with_tote` Ã© um ponto central de integraÃ§Ã£o entre pipelines, como visto no `packing_pipeline`, o que reforÃ§a a necessidade de consistÃªncia e limpeza nos dados de referÃªncia `olpn`.

---

### MÃ³dulo `logging_utils.py`

Este mÃ³dulo Ã© responsÃ¡vel pela configuraÃ§Ã£o centralizada do sistema de **log corporativo** dos pipelines de dados, garantindo rastreabilidade, diagnÃ³stico eficiente e rotaÃ§Ã£o controlada de arquivos.

### FunÃ§Ã£o: `setup_logging`

**Objetivo:**  
Configurar e retornar um logger padrÃ£o para todos os pipelines, com suporte a rotaÃ§Ã£o de arquivos e saÃ­da simultÃ¢nea no console.

**ParÃ¢metros:**
- `level: logging.Level` â†’ NÃ­vel de logging desejado (padrÃ£o: `logging.INFO`).

**Funcionamento:**
1. **ValidaÃ§Ã£o de Caminho:**
   - Garante que o diretÃ³rio definido em `LOG_PATH` (via `config.settings`) exista. Caso contrÃ¡rio, Ã© criado.

2. **InstanciaÃ§Ã£o do Logger:**
   - Um logger nomeado como `'pipeline_logger'` Ã© configurado com o nÃ­vel especificado.
   - O logger utiliza um formato padronizado de log:  
     `YYYY-MM-DD HH:MM:SS | LEVEL | MENSAGEM`

3. **RotaÃ§Ã£o de Arquivos:**
   - Usa `RotatingFileHandler` para gravar logs em disco.
   - Tamanho mÃ¡ximo por arquivo: `10 MB`
   - RetenÃ§Ã£o: atÃ© 5 arquivos de histÃ³rico (`backupCount=5`)

4. **SaÃ­da SimultÃ¢nea no Console:**
   - TambÃ©m adiciona um `StreamHandler` para exibir logs no terminal.

5. **Evita Duplicidade de Handlers:**
   - O logger sÃ³ adiciona handlers caso ainda nÃ£o existam, prevenindo mÃºltiplas saÃ­das duplicadas em reexecuÃ§Ãµes.

**Retorno:**
- Objeto `logger` jÃ¡ configurado, pronto para ser utilizado com `.info()`, `.warning()`, `.error()`, etc.

### Boas PrÃ¡ticas Corporativas Embutidas:

- **PersistÃªncia com rotaÃ§Ã£o:** evita crescimento descontrolado de arquivos de log.
- **Log estruturado e datado:** facilita leitura por humanos e integraÃ§Ã£o com ferramentas de observabilidade (ex: ELK, Splunk).
- **Console + arquivo:** habilita rastreabilidade local e arquivamento histÃ³rico simultaneamente.

---

### MÃ³dulo `transformation.py`

Este mÃ³dulo centraliza funÃ§Ãµes utilitÃ¡rias de transformaÃ§Ã£o e exportaÃ§Ã£o de dados, com foco em:
- PadronizaÃ§Ã£o de outputs mensais
- ConversÃ£o de formatos (CSV â†” Parquet)
- NormalizaÃ§Ã£o de valores categÃ³ricos com base em regras de negÃ³cio

---

### 1. `export_by_month(df, output_folder, pipeline_key)`

**Objetivo:**  
Exportar `DataFrame` particionado por mÃªs (`mes_ano`) em arquivos `.csv`, respeitando configuraÃ§Ãµes especÃ­ficas de cada pipeline.

**ParÃ¢metros:**
- `df`: `pd.DataFrame` contendo a coluna `mes_ano`
- `output_folder`: `Path` onde os arquivos serÃ£o salvos
- `pipeline_key`: chave de configuraÃ§Ã£o em `PIPELINE_CONFIG`

**CaracterÃ­sticas:**
- Valida a existÃªncia da configuraÃ§Ã£o
- Exporta com `utf-16` e `sep='\t'` (compatibilidade com Excel)
- Nome do arquivo: `dados_MM-AAAA.csv`
- Drop da coluna `mes_ano` no arquivo final

### MÃ³dulo `process_pipeline.py`

Este script orquestra a execuÃ§Ã£o completa do ecossistema de pipelines modulares do projeto, sendo o ponto de entrada principal para o processamento em lote dos dados operacionais.

---

### ğŸ“Œ Objetivo Geral

Consolidar e padronizar a execuÃ§Ã£o sequencial dos seguintes fluxos:

- **Pipelines baseados em `BasePipeline`**: processam arquivos CSV brutos em estruturas normalizadas e particionadas
- **Pipelines autogerenciados (integrados)**: realizam agregaÃ§Ãµes, cruzamentos e cÃ¡lculos compostos entre mÃºltiplas fontes jÃ¡ tratadas

---

### ğŸ” Fluxo de ExecuÃ§Ã£o

#### 1. **InicializaÃ§Ã£o**

- Configura o `logger`
- Define `start_time` para mensuraÃ§Ã£o de performance
- Carrega `PIPELINE_PATHS` para leitura e escrita de arquivos

---

#### 2. **ExecuÃ§Ã£o dos Pipelines Base**

Executa os seguintes pipelines, herdando de `BasePipeline`:

| Pipeline    | FunÃ§Ã£o                                    |
|-------------|--------------------------------------------|
| `olpn`      | Processamento de dados de OLPN            |
| `picking`   | AnÃ¡lise de tarefas de separaÃ§Ã£o           |
| `cancel`    | NormalizaÃ§Ã£o e classificaÃ§Ã£o de cancelamentos |
| `load`      | IdentificaÃ§Ã£o de carga e tempos por box   |
| `putaway`   | Armazenamento e tempo por box             |
| `packing`   | Empacotamento e tempos por tote/box       |

**Para cada pipeline:**
- Caminhos de entrada, saÃ­da `.csv` e `.parquet` sÃ£o resolvidos via `PIPELINE_PATHS`
- DiretÃ³rios sÃ£o criados caso nÃ£o existam
- Pipeline Ã© executado via `.run()`
- Resultado Ã© validado (`df.empty`) e logado

---

#### 3. **ExecuÃ§Ã£o dos Pipelines Integrados**

Executa pipelines com estrutura independente (nÃ£o usam `BasePipeline`), projetados para lÃ³gica de negÃ³cio composta:

| Pipeline               | DescriÃ§Ã£o                                                        |
|------------------------|------------------------------------------------------------------|
| `JornadaPipeline`      | Consolida jornada logÃ­stica por OLPN (integra vÃ¡rios pipelines) |
| `TimeLeadOLPNPipeline` | Calcula tempo de ciclo entre eventos por OLPN                   |
| `BottleneckSalao`      | Identifica gargalos operacionais no "salÃ£o" logÃ­stico          |
| `BottleneckBox`        | Analisa tempo de retenÃ§Ã£o por `box` entre `load` e `putaway`   |

---

### ğŸ§  Boas PrÃ¡ticas Aplicadas

- **ModularizaÃ§Ã£o extensÃ­vel** com uso de `BasePipeline` e pipelines autÃ´nomos
- **OrquestraÃ§Ã£o robusta e tolerante a falhas**
- **Monitoramento completo via logging estruturado**
- **Particionamento por `mes_ano` e persistÃªncia eficiente em `.parquet`**

---

### â±ï¸ MÃ©tricas de Performance

- O tempo total de execuÃ§Ã£o Ã© registrado e logado com `time.time()`
- Permite anÃ¡lise posterior de SLA do pipeline completo

---