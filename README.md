# üì¶ Data Pipeline

Projeto robusto de pipelines de dados para tratamento, transforma√ß√£o e an√°lise de dados log√≠sticos. Estruturado de forma modular, com foco em reutiliza√ß√£o de c√≥digo, logging centralizado e escalabilidade, o **Data Pipeline** viabiliza a ingest√£o, limpeza, processamento e integra√ß√£o de dados operacionais, com suporte a m√∫ltiplos fluxos simult√¢neos.

## üìÅ Estrutura do Projeto

![image](https://github.com/user-attachments/assets/7588a9e7-1c38-4a08-8559-7bf7ad972efc)

---

## üéØ Objetivo

Automatizar o tratamento e an√°lise de dados operacionais log√≠sticos, garantindo consist√™ncia, rastreabilidade e performance no fluxo de ponta a ponta, desde a ingest√£o de arquivos brutos at√© a gera√ß√£o de arquivos Parquet prontos para an√°lise em ferramentas de BI.

---

## Vis√£o Geral

### M√≥dulo `settings.py`

Este m√≥dulo centraliza as configura√ß√µes globais e espec√≠ficas de cada pipeline do projeto **Data Pipeline**, incluindo caminhos de dados, par√¢metros de leitura e escrita, defini√ß√µes de tipos de dados, regras de transforma√ß√£o e mapeamentos de neg√≥cio. Atua como fonte √∫nica de verdade para os pipelines, garantindo coer√™ncia e facilidade de manuten√ß√£o.

---

### M√≥dulo `bottleneck_box.py`

O m√≥dulo `bottleneck_box.py` implementa o pipeline dedicado √† an√°lise do gargalo operacional denominado "bottleneck box". Utilizando leitura eficiente de arquivos Parquet particionados, o pipeline integra dados dos est√°gios de `load` e `putaway`, aplicando regras de tratamento de dados configuradas centralmente para garantir padroniza√ß√£o e qualidade.

A l√≥gica central consiste em combinar os datasets via chave √∫nica (`olpn`), realizar convers√µes temporais para c√°lculo do tempo entre eventos cr√≠ticos, e exportar os resultados segmentados por m√™s para facilitar an√°lises temporais e gera√ß√£o de relat√≥rios. Todo processo √© monitorado via logging robusto, assegurando rastreabilidade e facilidade de diagn√≥stico em ambiente produtivo.

---

### M√≥dulo `bottleneck_salao.py`

O m√≥dulo `bottleneck_salao.py` implementa o pipeline voltado para an√°lise do gargalo operacional denominado "bottleneck sal√£o". O pipeline realiza a leitura eficiente de arquivos Parquet particionados referentes √†s etapas de `packed` e `putaway`, consolidando os dados com base na chave √∫nica `olpn`.

Atrav√©s da uni√£o dos datasets e convers√£o de colunas temporais, calcula o intervalo entre eventos cr√≠ticos para identificar e mensurar o gargalo. Os resultados s√£o exportados em arquivos segmentados mensalmente, facilitando an√°lises temporais e o monitoramento cont√≠nuo. Todo o processo √© suportado por um sistema de logging estruturado, garantindo rastreabilidade e confiabilidade operacional.

---

### M√≥dulo `jornada_pipeline.py`

O m√≥dulo `jornada_pipeline.py` implementa o pipeline focado no processamento de dados de jornada de trabalho. Ele realiza o pr√©-processamento estruturado de arquivos CSV, aplicando limpeza, padroniza√ß√£o e enriquecimento dos dados conforme configura√ß√µes espec√≠ficas do pipeline.

Al√©m disso, converte arquivos CSV processados para o formato Parquet, otimizando armazenamento e performance para an√°lises subsequentes. O pipeline suporta segmenta√ß√£o dos dados por m√™s para facilitar a gest√£o temporal e conta com logging detalhado para garantir monitoramento e r√°pida identifica√ß√£o de falhas durante a execu√ß√£o.

---

### M√≥dulo `time_lead_olpn.py`

O m√≥dulo `time_lead_olpn.py` implementa o pipeline respons√°vel pela an√°lise do lead time do processo OLPN. Ele realiza a leitura de m√∫ltiplos arquivos Parquet, aplicando pr√©-processamento com valida√ß√£o, limpeza, tratamento de tipos e c√°lculo do tempo absoluto entre eventos-chave (`data_hora_load` e `data_pedido`).

Os dados s√£o segmentados e exportados por m√™s, garantindo granularidade temporal para an√°lises operacionais e de performance. A solu√ß√£o incorpora logging avan√ßado para rastreamento detalhado, assegurando transpar√™ncia e controle durante toda a execu√ß√£o do pipeline.

---

### M√≥dulo `base_pipeline.py`

O m√≥dulo `base_pipeline.py` define a classe abstrata `BasePipeline`, que serve como estrutura base para a implementa√ß√£o dos pipelines espec√≠ficos do projeto. Ela encapsula a l√≥gica central de leitura de arquivos CSV, pr√©-processamento, valida√ß√£o e exporta√ß√£o dos dados segmentados por m√™s, tanto em CSV quanto em Parquet.

A classe promove padroniza√ß√£o e reutiliza√ß√£o, exigindo que subclasses implementem o m√©todo `preprocess` para aplicar regras espec√≠ficas de transforma√ß√£o. O uso de logging robusto assegura controle operacional e f√°cil diagn√≥stico durante a execu√ß√£o dos pipelines.

---

### M√≥dulo `cancel_pipeline.py`

Este m√≥dulo implementa o `CancelPipeline`, uma extens√£o da classe base `BasePipeline`, dedicada ao processamento dos dados de cancelamento.

A pipeline executa etapas estruturadas de limpeza e transforma√ß√£o, incluindo:
- Remo√ß√£o e renomea√ß√£o de colunas conforme configura√ß√£o espec√≠fica.
- Convers√£o de tipos de dados e normaliza√ß√£o de datas.
- Classifica√ß√£o dos setores com fun√ß√£o customizada `cancel_classify_setores`.
- Tratamento avan√ßado da coluna `motivo_cancelamento`, aplicando normaliza√ß√£o textual (remo√ß√£o de acentos, padroniza√ß√£o de caixa) e categoriza√ß√£o via a fun√ß√£o `normalizar_motivo`.
- Gera√ß√£o de colunas auxiliares para an√°lise temporal (`hora`, `data_criterio`, `mes_ano`) baseadas na data do cancelamento.

Essa arquitetura promove governan√ßa robusta, assegurando qualidade e consist√™ncia dos dados para an√°lises avan√ßadas e dashboards corporativos.

---

### M√≥dulo `load_pipeline.py`

Este m√≥dulo define o `LoadPipeline`, uma especializa√ß√£o da classe base `BasePipeline` focada no tratamento dos dados de carregamento (`load`).

O processo de pr√©-processamento contempla:

- Limpeza estruturada removendo e renomeando colunas conforme configura√ß√£o centralizada.
- Convers√£o rigorosa de tipos e tratamento de colunas temporais.
- Extra√ß√£o da data base a partir da coluna `data_hora_load`.
- C√°lculo do tempo total de atividade di√°ria por box (`duracao_segundos_box_dia`), agregando as m√≠nimas e m√°ximas marca√ß√µes temporais por combina√ß√£o de box e data.
- Gera√ß√£o de colunas temporais padronizadas para an√°lises granulares (`hora`, `data_criterio`, `mes_ano`).
- Classifica√ß√£o dos setores com fun√ß√£o dedicada `classify_setores`.
- Limpeza final removendo colunas auxiliares resultantes de merges intermedi√°rios.

A pipeline refor√ßa a governan√ßa dos dados operacionais, garantindo a consist√™ncia e enriquecimento temporal para an√°lises avan√ßadas e monitoramento estrat√©gico.  

---

### M√≥dulo `olpn_pipeline.py`

Este m√≥dulo implementa a classe `OlpnPipeline`, derivada de `BasePipeline`, destinada ao pr√©-processamento dos dados OPLN (Order Picking Location Number).

Pontos-chave do processamento:

- Remo√ß√£o e renomea√ß√£o de colunas conforme configura√ß√£o centralizada.
- Convers√£o robusta de tipos, incluindo tratamento especial para a coluna `volume` com convers√£o num√©rica e substitui√ß√£o de v√≠rgulas.
- Extra√ß√£o detalhada do campo `local_de_picking` em tr√™s componentes (`rua`, `endereco` e `nivel`), facilitando an√°lises granulares de localiza√ß√£o.
- Classifica√ß√£o de setores por meio da fun√ß√£o `classify_setores`.
- Constru√ß√£o da coluna `localizacao` com l√≥gica condicional baseada em valores espec√≠ficos, distinguindo entre √°rea "P.A.R" e "Salao".
- Defini√ß√£o de parti√ß√µes temporais (`mes_ano`, `data_criterio`, `hora`) fundamentadas na coluna `data_hora_ultimo_update_olpn`, com fallback controlado para aus√™ncia desse dado.

Essa pipeline √© essencial para estruturar, enriquecer e segmentar os dados log√≠sticos, habilitando an√°lises espaciais e temporais avan√ßadas dentro do contexto operacional.

---

### M√≥dulo `packing_pipeline.py`

Este m√≥dulo implementa a classe `PackingPipeline`, que herda de `BasePipeline` e √© respons√°vel pelo processamento dos dados de packing, com integra√ß√£o da refer√™ncia OLPN para enriquecer o dataset.

Destaques do processamento:

- Carregamento da base OLPN via fun√ß√£o especializada `read_parquet_with_tote` para integra√ß√£o de dados.
- Aplica√ß√£o de transforma√ß√µes configuradas, incluindo remo√ß√£o e renomea√ß√£o de colunas, ajuste de tipos e normaliza√ß√£o textual da coluna `desc_setor_item`.
- Merge entre os dados de packing e a refer√™ncia OLPN com chave `olpn`, agregando informa√ß√µes cruciais.
- C√°lculo do tempo de atividade di√°rio por `box` e por `tote` baseado em timestamps, gerando m√©tricas de dura√ß√£o em segundos.
- Ajuste para garantir que dura√ß√µes de tote menores que 60 segundos sejam normalizadas para 60 segundos, assegurando consist√™ncia anal√≠tica.
- Extra√ß√£o e formata√ß√£o de parti√ß√µes temporais (`hora`, `data_criterio`, `mes_ano`) a partir da coluna `data_hora_packed`.
- Classifica√ß√£o de setores para segmenta√ß√£o operacional.
- Limpeza final dos dados para eliminar colunas intermedi√°rias irrelevantes.

Essa pipeline garante uma vis√£o integrada e temporalmente segmentada do processo de packing, essencial para an√°lises de efici√™ncia operacional e controle log√≠stico.

---

### M√≥dulo `picking_pipeline.py`

Este m√≥dulo define a classe `PickingPipeline`, especializada no processamento dos dados de picking, derivando da classe base `BasePipeline`.

Principais funcionalidades e transforma√ß√µes:

- Aplica√ß√£o de limpeza e padroniza√ß√£o via remo√ß√£o e renomea√ß√£o de colunas conforme configura√ß√£o.
- Convers√£o de tipos de dados e parsing de datas para garantir integridade e coer√™ncia temporal.
- C√°lculo do tempo de dura√ß√£o da tarefa (`duracao_tarefa_segundos`) a partir das colunas de in√≠cio e fim da tarefa.
- Desmembramento da coluna `local_de_picking` em subcomponentes (`rua`, `endereco`, `nivel`) para granularidade espacial.
- Preenchimento de valores ausentes nos campos de localiza√ß√£o para evitar inconsist√™ncias.
- Classifica√ß√£o da localiza√ß√£o em categorias operacionais ("P.A.R" ou "Salao") baseada em regras definidas para ruas e endere√ßos.
- Atribui√ß√£o de setores via fun√ß√£o de classifica√ß√£o espec√≠fica, promovendo segmenta√ß√£o anal√≠tica.
- Gera√ß√£o de colunas temporais de particionamento (`mes_ano`, `data_criterio`, `hora`) baseadas no timestamp de t√©rmino da tarefa.
- Logging de alertas quando informa√ß√µes essenciais para particionamento temporal est√£o ausentes.

Esta pipeline promove a an√°lise detalhada do desempenho e efici√™ncia das opera√ß√µes de picking, com foco em segmenta√ß√£o espacial e temporal para suportar decis√µes estrat√©gicas e t√°ticas.

---

### M√≥dulo `putaway_pipeline.py`

Este m√≥dulo implementa a classe `PutawayPipeline`, especializada no processamento dos dados de putaway, estendendo a estrutura do `BasePipeline`.

Pontos-chave da pipeline:

- Aplica√ß√£o de limpeza, remo√ß√£o e renomea√ß√£o de colunas conforme definido nas configura√ß√µes espec√≠ficas.
- Convers√£o robusta de tipos, incluindo extra√ß√£o num√©rica da coluna `box` para padroniza√ß√£o.
- Parsing e valida√ß√£o das colunas de data/hora para garantir coer√™ncia temporal.
- C√°lculo do tempo total por box e dia, a partir das timestamps m√≠nimas e m√°ximas, refletindo a dura√ß√£o operacional di√°ria.
- Integra√ß√£o dos c√°lculos de tempo com o DataFrame principal via merge, para an√°lise consolidada.
- Constru√ß√£o de colunas para particionamento temporal (`hora`, `data_criterio`, `mes_ano`) com base no timestamp de putaway.
- Classifica√ß√£o dos dados em setores operacionais por meio da fun√ß√£o `classify_setores`.
- Limpeza final removendo colunas tempor√°rias ou redundantes para otimizar o dataset.
- Logging para monitoramento do fluxo e alertas sobre aus√™ncia de dados cr√≠ticos.

Esta pipeline √© essencial para mensurar e analisar a efici√™ncia das opera√ß√µes de putaway, suportando decis√µes que impactam diretamente a produtividade e a gest√£o do fluxo log√≠stico.

---

### M√≥dulo `classification.py`

Este m√≥dulo cont√©m fun√ß√µes essenciais para classifica√ß√£o setorial dos dados operacionais, fundamental para segmenta√ß√£o anal√≠tica e tomada de decis√£o orientada por contexto.

### Fun√ß√£o `classify_setores`

- Recebe um DataFrame com informa√ß√µes de pedidos e boxes.
- Aplica m√∫ltiplas condi√ß√µes l√≥gicas compostas por combina√ß√µes de `tipo_de_pedido` e faixa num√©rica de `box`.
- Cada condi√ß√£o associa um r√≥tulo setorial espec√≠fico (ex: "Fracionado Pesados", "EAD - Abastecimento de Lojas", "Ribeirao Preto", etc.).
- Converte a coluna `box` para inteiro, tratando valores inv√°lidos.
- Retorna uma s√©rie com a classifica√ß√£o setorial baseada nas condi√ß√µes.
- Valor default para registros que n√£o atendem nenhuma condi√ß√£o √© "Outras Saidas".

### Fun√ß√£o `cancel_classify_setores`

- Similar √† fun√ß√£o anterior, mas focada em classifica√ß√£o para dados de cancelamento.
- Avalia apenas o campo `tipo_de_pedido` para atribuir os setores.
- Tamb√©m retorna uma s√©rie categorizada, com default "Outras saidas".

### M√≥dulo `io.py`

Este m√≥dulo √© respons√°vel pela leitura eficiente de arquivos de entrada nos pipelines, garantindo resili√™ncia, performance e consist√™ncia com as configura√ß√µes de cada fluxo de dados.

### Fun√ß√£o: `read_all_csv`

**Objetivo:**  
Ler todos os arquivos `.csv` de um diret√≥rio espec√≠fico, utilizando `chunks` para efici√™ncia de mem√≥ria.

**Par√¢metros:**
- `folder: Path` ‚Üí Caminho para a pasta contendo os arquivos `.csv`.
- `pipeline_key: str` ‚Üí Chave de identifica√ß√£o do pipeline no `PIPELINE_CONFIG`.

**Funcionamento:**
- Recupera configura√ß√µes espec√≠ficas de encoding e separador do pipeline.
- Itera por todos os arquivos `.csv` no diret√≥rio, utilizando `pd.read_csv()` com `chunksize` (carregamento em blocos).
- Todos os chunks s√£o concatenados ao final em um √∫nico `DataFrame`.
- Em caso de erro durante a leitura de qualquer arquivo, registra no log e prossegue com os demais.

**Retorno:**
- `pd.DataFrame` consolidado contendo os dados de todos os arquivos `.csv`.

### Fun√ß√£o: `read_parquet_with_tote`

**Objetivo:**  
Ler m√∫ltiplos arquivos `.parquet` contendo apenas as colunas `olpn` e `tote`.

**Par√¢metros:**
- `folder: Path` ‚Üí Caminho da pasta com arquivos `.parquet`.

**Funcionamento:**
- Itera sobre os arquivos `.parquet` da pasta.
- Para cada arquivo, l√™ somente as colunas `['olpn', 'tote']`.
- Concatena os `DataFrames` e remove duplicatas com base em `olpn`.

**Retorno:**
- `pd.DataFrame` √∫nico contendo os pares `olpn`-`tote` sem duplica√ß√µes.

### Observa√ß√µes Estrat√©gicas

- O uso de leitura por chunks (`CHUNK_SIZE`) no `read_all_csv` √© uma pr√°tica robusta para evitar estouro de mem√≥ria em arquivos extensos.
- Ambos os m√©todos s√£o projetados para integrar-se diretamente ao sistema de configura√ß√£o modular `PIPELINE_CONFIG`, conferindo escalabilidade e adaptabilidade.
- O `read_parquet_with_tote` √© um ponto central de integra√ß√£o entre pipelines, como visto no `packing_pipeline`, o que refor√ßa a necessidade de consist√™ncia e limpeza nos dados de refer√™ncia `olpn`.

---

### M√≥dulo `logging_utils.py`

Este m√≥dulo √© respons√°vel pela configura√ß√£o centralizada do sistema de **log corporativo** dos pipelines de dados, garantindo rastreabilidade, diagn√≥stico eficiente e rota√ß√£o controlada de arquivos.

### Fun√ß√£o: `setup_logging`

**Objetivo:**  
Configurar e retornar um logger padr√£o para todos os pipelines, com suporte a rota√ß√£o de arquivos e sa√≠da simult√¢nea no console.

**Par√¢metros:**
- `level: logging.Level` ‚Üí N√≠vel de logging desejado (padr√£o: `logging.INFO`).

**Funcionamento:**
1. **Valida√ß√£o de Caminho:**
   - Garante que o diret√≥rio definido em `LOG_PATH` (via `config.settings`) exista. Caso contr√°rio, √© criado.

2. **Instancia√ß√£o do Logger:**
   - Um logger nomeado como `'pipeline_logger'` √© configurado com o n√≠vel especificado.
   - O logger utiliza um formato padronizado de log:  
     `YYYY-MM-DD HH:MM:SS | LEVEL | MENSAGEM`

3. **Rota√ß√£o de Arquivos:**
   - Usa `RotatingFileHandler` para gravar logs em disco.
   - Tamanho m√°ximo por arquivo: `10 MB`
   - Reten√ß√£o: at√© 5 arquivos de hist√≥rico (`backupCount=5`)

4. **Sa√≠da Simult√¢nea no Console:**
   - Tamb√©m adiciona um `StreamHandler` para exibir logs no terminal.

5. **Evita Duplicidade de Handlers:**
   - O logger s√≥ adiciona handlers caso ainda n√£o existam, prevenindo m√∫ltiplas sa√≠das duplicadas em reexecu√ß√µes.

**Retorno:**
- Objeto `logger` j√° configurado, pronto para ser utilizado com `.info()`, `.warning()`, `.error()`, etc.

### Boas Pr√°ticas Corporativas Embutidas:

- **Persist√™ncia com rota√ß√£o:** evita crescimento descontrolado de arquivos de log.
- **Log estruturado e datado:** facilita leitura por humanos e integra√ß√£o com ferramentas de observabilidade (ex: ELK, Splunk).
- **Console + arquivo:** habilita rastreabilidade local e arquivamento hist√≥rico simultaneamente.

---

### M√≥dulo `transformation.py`

Este m√≥dulo centraliza fun√ß√µes utilit√°rias de transforma√ß√£o e exporta√ß√£o de dados, com foco em:
- Padroniza√ß√£o de outputs mensais
- Convers√£o de formatos (CSV ‚Üî Parquet)
- Normaliza√ß√£o de valores categ√≥ricos com base em regras de neg√≥cio

---

### 1. `export_by_month(df, output_folder, pipeline_key)`

**Objetivo:**  
Exportar `DataFrame` particionado por m√™s (`mes_ano`) em arquivos `.csv`, respeitando configura√ß√µes espec√≠ficas de cada pipeline.

**Par√¢metros:**
- `df`: `pd.DataFrame` contendo a coluna `mes_ano`
- `output_folder`: `Path` onde os arquivos ser√£o salvos
- `pipeline_key`: chave de configura√ß√£o em `PIPELINE_CONFIG`

**Caracter√≠sticas:**
- Valida a exist√™ncia da configura√ß√£o
- Exporta com `utf-16` e `sep='\t'` (compatibilidade com Excel)
- Nome do arquivo: `dados_MM-AAAA.csv`
- Drop da coluna `mes_ano` no arquivo final

### M√≥dulo `process_pipeline.py`

Este script orquestra a execu√ß√£o completa do ecossistema de pipelines modulares do projeto, sendo o ponto de entrada principal para o processamento em lote dos dados operacionais.

---

### üìå Objetivo Geral

Consolidar e padronizar a execu√ß√£o sequencial dos seguintes fluxos:

- **Pipelines baseados em `BasePipeline`**: processam arquivos CSV brutos em estruturas normalizadas e particionadas
- **Pipelines autogerenciados (integrados)**: realizam agrega√ß√µes, cruzamentos e c√°lculos compostos entre m√∫ltiplas fontes j√° tratadas

---

### üîÅ Fluxo de Execu√ß√£o

#### 1. **Inicializa√ß√£o**

- Configura o `logger`
- Define `start_time` para mensura√ß√£o de performance
- Carrega `PIPELINE_PATHS` para leitura e escrita de arquivos

---

#### 2. **Execu√ß√£o dos Pipelines Base**

Executa os seguintes pipelines, herdando de `BasePipeline`:

| Pipeline    | Fun√ß√£o                                    |
|-------------|--------------------------------------------|
| `olpn`      | Processamento de dados de OLPN            |
| `picking`   | An√°lise de tarefas de separa√ß√£o           |
| `cancel`    | Normaliza√ß√£o e classifica√ß√£o de cancelamentos |
| `load`      | Identifica√ß√£o de carga e tempos por box   |
| `putaway`   | Armazenamento e tempo por box             |
| `packing`   | Empacotamento e tempos por tote/box       |

**Para cada pipeline:**
- Caminhos de entrada, sa√≠da `.csv` e `.parquet` s√£o resolvidos via `PIPELINE_PATHS`
- Diret√≥rios s√£o criados caso n√£o existam
- Pipeline √© executado via `.run()`
- Resultado √© validado (`df.empty`) e logado

---

#### 3. **Execu√ß√£o dos Pipelines Integrados**

Executa pipelines com estrutura independente (n√£o usam `BasePipeline`), projetados para l√≥gica de neg√≥cio composta:

| Pipeline               | Descri√ß√£o                                                        |
|------------------------|------------------------------------------------------------------|
| `JornadaPipeline`      | Consolida jornada log√≠stica por OLPN (integra v√°rios pipelines) |
| `TimeLeadOLPNPipeline` | Calcula tempo de ciclo entre eventos por OLPN                   |
| `BottleneckSalao`      | Identifica gargalos operacionais no "sal√£o" log√≠stico          |
| `BottleneckBox`        | Analisa tempo de reten√ß√£o por `box` entre `load` e `putaway`   |

---

### üß† Boas Pr√°ticas Aplicadas

- **Modulariza√ß√£o extens√≠vel** com uso de `BasePipeline` e pipelines aut√¥nomos
- **Orquestra√ß√£o robusta e tolerante a falhas**
- **Monitoramento completo via logging estruturado**
- **Particionamento por `mes_ano` e persist√™ncia eficiente em `.parquet`**

---

### ‚è±Ô∏è M√©tricas de Performance

- O tempo total de execu√ß√£o √© registrado e logado com `time.time()`
- Permite an√°lise posterior de SLA do pipeline completo

---
